{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa67b8d0-f013-4ba0-a615-12247bfc9ac7",
   "metadata": {},
   "source": [
    "# Model Prototyping\n",
    "\n",
    "Building the basis for our model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753b036f-c5b8-44cc-a653-4712b3b19d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.nn import Conv2d, AvgPool2d, ReLU, Dropout, Flatten, Linear, Sequential, Module\n",
    "from torch.optim import Adam\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "MODELS_DIR  = '/home/cxw/sonos_rirs/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e62a78-bba4-453b-9961-dba6aa7ba962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "model_dict['name'] = \"testrun2_regularization\"\n",
    "model_dict['notes'] = \"same as test run but with regularization\"\n",
    "model_dict['data_path'] = '/home/cxw/sonos_rirs/features/080122_5k_phase/feature_df.csv'\n",
    "model_dict['model_path'] = os.path.join(MODELS_DIR, model_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 尝试导入IPython\n",
    "    from IPython import get_ipython\n",
    "    # 检查是否在IPython环境下\n",
    "    if get_ipython() is not None:\n",
    "        # 加载autoreload扩展\n",
    "        %load_ext autoreload\n",
    "        # 设置autoreload为2\n",
    "        %autoreload 2\n",
    "except ImportError:\n",
    "    # 如果IPython没有被安装，则不作任何操作\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e13f9ea-8e81-447e-b228-3c1fa0b22de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %autoreload 2\n",
    "# # import volume_estimation.modeling as model_funcs\n",
    "# model_funcs.train_model(model_funcs.Baseline_Model, model_dict,\\\n",
    "#                         overwrite=True, epochs=1,log=False) #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723b0f82-6ce6-489f-9c5a-10161015d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32000it [00:19, 1600.44it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_df = pd.read_csv(model_dict['data_path'])\n",
    "model_path = os.path.join(MODELS_DIR, model_dict['name'])\n",
    "\n",
    "dataset = []\n",
    "\n",
    "    \n",
    "def create_dataloader(feature_df, batch_size=1, log=True):\n",
    "    dataset = []\n",
    "    for row in tqdm(feature_df.iterrows()):\n",
    "        feat_file = row[1]['file_feature']\n",
    "        loaded = np.load(feat_file)\n",
    "\n",
    "        feature = loaded['feat']\n",
    "        feature = feature.reshape((feature.shape[0], feature.shape[1]))\n",
    "        feature = np.real(feature)\n",
    "\n",
    "        vol = loaded['vol']\n",
    "        if log:\n",
    "            vol = np.log10(vol)\n",
    "        dataset.append((feature, vol))\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(feat_df, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c987b41d-7f00-41cc-bb96-c6fdedc585f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name testrun2_regularization\n",
      "notes same as test run but with regularization\n",
      "data_path /home/cxw/sonos_rirs/features/080122_5k_phase/feature_df.csv\n",
      "model_path /home/cxw/sonos_rirs/models/testrun2_regularization\n"
     ]
    }
   ],
   "source": [
    "savename = './testmodeldict.json'\n",
    "with open(savename, 'w') as f:\n",
    "    json.dump(model_dict, f)\n",
    "    \n",
    "with open(savename) as f:\n",
    "    load_dict = json.load(f)\n",
    "    \n",
    "for key in load_dict.keys():\n",
    "    print(key, load_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a66ab9-e949-4408-943c-ade51b8c2562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25620it [00:15, 1617.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6380it [00:04, 1583.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6380it [00:03, 1599.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = feat_df[feat_df['split'].isin(['train','val'])]\n",
    "val_df = feat_df[feat_df['split']=='test']\n",
    "test_df = feat_df[feat_df['split']=='test']\n",
    "\n",
    "print(\"Creating training dataloader\")\n",
    "train_dataloader = create_dataloader(train_df, batch_size=16)        ##################################batch_size\n",
    "\n",
    "print(\"Creating validation dataloader\")\n",
    "val_dataloader = create_dataloader(val_df)\n",
    "\n",
    "print(\"Creating test dataloader\")\n",
    "test_dataloader = create_dataloader(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f680db8-27a2-4db8-84d3-2ef287da00ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([16, 30, 1997])\n",
      "Labels batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "# features = features.squeeze(1)\n",
    "# features = features.transpose(1,2)\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c776c9f2-fba2-4f0f-9ea2-93f35ecaeb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Baseline_Model(Module):\n",
    "#     def __init__(self, input_shape):\n",
    "#         #accepts a tuple with the height/width of the feature\n",
    "#         #matrix to set the FC layer dimensions\n",
    "#         super(Baseline_Model, self).__init__()\n",
    "        \n",
    "#         #block1\n",
    "#         Conv1 = Conv2d(1, 30, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool1 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block2\n",
    "#         Conv2 = Conv2d(30, 20, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool2 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block3\n",
    "#         Conv3 = Conv2d(20, 10, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool3 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block4\n",
    "#         Conv4 = Conv2d(10, 10, kernel_size=(1,10), stride=(1,1))\n",
    "#         Avgpool4 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block5\n",
    "#         Conv5 = Conv2d(10, 5, kernel_size=(3,9), stride=(1,1))\n",
    "#         Avgpool5 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "#         #block6\n",
    "#         Conv6 = Conv2d(5, 5, kernel_size=(3,9), stride=(1,1))\n",
    "#         Avgpool6 = AvgPool2d((2,2), stride=(2,2))\n",
    "\n",
    "#         #dropout\n",
    "#         dropout_layer = Dropout(p=0.5)\n",
    "#         height5 = input_shape[0] - 2\n",
    "#         height6 = (height5 - 2) // 2\n",
    "\n",
    "#         time1 = (input_shape[1] - 9) // 2\n",
    "#         time2 = (time1 - 9) // 2\n",
    "#         time3 = (time2 - 9) // 2\n",
    "#         time4 = (time3 - 9) // 2\n",
    "#         time5 = (time4 - 7) // 2\n",
    "#         time6 = (time5 - 7) // 2\n",
    "\n",
    "#         flat_dims = 5 * height6 * time6\n",
    "#         fc_layer = Linear(flat_dims, 1)\n",
    "        \n",
    "#         self.net = Sequential(\n",
    "#                     Conv1, ReLU(), Avgpool1,\n",
    "#                     Conv2, ReLU(), Avgpool2,\n",
    "#                     Conv3, ReLU(), Avgpool3,\n",
    "#                     Conv4, ReLU(), Avgpool4,\n",
    "#                     Conv5, ReLU(), Avgpool5,\n",
    "#                     Conv6, ReLU(), Avgpool6,\n",
    "#                     dropout_layer, Flatten(),\n",
    "#                     fc_layer, Flatten()\n",
    "#                 )\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.net:\n",
    "#             x = layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e1d4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=16, time stride=16\n",
      "number of patches=124\n",
      "torch.Size([10, 1996, 30])\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.9002],\n",
      "        [0.7522],\n",
      "        [0.8259],\n",
      "        [0.7624],\n",
      "        [0.8778],\n",
      "        [0.7921],\n",
      "        [0.8861],\n",
      "        [0.8130],\n",
      "        [0.8627],\n",
      "        [0.7967]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 6/10/21 5:04 PM\n",
    "# @Author  : Yuan Gong\n",
    "# @Affiliation  : Massachusetts Institute of Technology\n",
    "# @Email   : yuangong@mit.edu\n",
    "# @File    : ast_models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import wget\n",
    "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "\n",
    "# override the timm package to relax the input shape constraint.\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=1, fstride=16, tstride=16, input_fdim=30, input_tdim=1996, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
    "\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        # override timm input shape restriction\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "\n",
    "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
    "        if audioset_pretrain == False:\n",
    "            if model_size == 'tiny224':\n",
    "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'small224':\n",
    "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base224':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base384':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
    "            else:\n",
    "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
    "            self.original_num_patches = self.v.patch_embed.num_patches\n",
    "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            # automatcially get the intermediate shape\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            # the linear projection layer\n",
    "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "            if imagenet_pretrain == True:\n",
    "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
    "                new_proj.bias = self.v.patch_embed.proj.bias\n",
    "            self.v.patch_embed.proj = new_proj\n",
    "\n",
    "            # the positional embedding\n",
    "            if imagenet_pretrain == True:\n",
    "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
    "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
    "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
    "                if t_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
    "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
    "                if f_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "                # flatten the positional embedding\n",
    "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
    "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
    "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "            else:\n",
    "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "                # TODO can use sinusoidal positional embedding instead\n",
    "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "                self.v.pos_embed = new_pos_embed\n",
    "                trunc_normal_(self.v.pos_embed, std=.02)\n",
    "\n",
    "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
    "        elif audioset_pretrain == True:\n",
    "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
    "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
    "            if model_size != 'base384':\n",
    "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
    "                # this model performs 0.4593 mAP on the audioset eval set\n",
    "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "                wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n",
    "            sd = torch.load('../../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
    "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
    "            audio_model = torch.nn.DataParallel(audio_model)\n",
    "            audio_model.load_state_dict(sd, strict=False)\n",
    "            self.v = audio_model.module.v\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
    "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
    "            if t_dim < 101:\n",
    "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
    "            # otherwise interpolate\n",
    "            else:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
    "            if f_dim < 12:\n",
    "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
    "            # otherwise interpolate\n",
    "            elif f_dim > 12:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
    "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        for blk in self.v.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.v.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # input_tdim = 100\n",
    "    # ast_mdl = ASTModel(input_tdim=input_tdim)\n",
    "    # # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
    "    # test_input = torch.rand([10, input_tdim, 128])\n",
    "    # test_output = ast_mdl(test_input)\n",
    "    # # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
    "    # print(test_output.shape)\n",
    "\n",
    "    input_tdim = 1996\n",
    "    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=1, audioset_pretrain=False)\n",
    "    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
    "    test_input = torch.rand([10, input_tdim, 30])\n",
    "    print(test_input.shape)\n",
    "    test_output = ast_mdl(test_input)\n",
    "    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
    "    print(test_output.shape)\n",
    "    print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "679cdf44-c94c-4f28-85ef-60136ca87ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=16, time stride=16\n",
      "number of patches=124\n"
     ]
    }
   ],
   "source": [
    "input_height = features.size()[1]\n",
    "input_width = features.size()[2]\n",
    "model = ASTModel(input_tdim=input_width,input_fdim=input_height,label_dim=1, imagenet_pretrain=True, audioset_pretrain=False).to(device)\n",
    "# model = Baseline_Model((input_height, input_width)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15e54c8-8449-4890-95ee-a5749982a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "def Bias(output, target):\n",
    "    loss = torch.mean(torch.abs(10**output - 10**target))\n",
    "    return loss\n",
    "\n",
    "def CovStep(output, target, output_mean, target_mean):\n",
    "    loss = torch.mean(((output - output_mean) * (target - target_mean)))\n",
    "    return loss\n",
    "\n",
    "def MeanAbsLogStep(output, target, log=True):\n",
    "    #convert out of log\n",
    "    if log:\n",
    "        vol_pred = 10**output\n",
    "        vol_target = 10**target\n",
    "    else:\n",
    "        vol_pred = output\n",
    "        vol_target = target\n",
    "    loss = torch.mean(torch.abs(torch.log(vol_pred/vol_target)))\n",
    "    return loss\n",
    "\n",
    "def compute_eval_metrics(dataloader, model, log=True):\n",
    "    target_sum = 0\n",
    "    pred_sum = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        target_sum += y.sum()\n",
    "        pred_sum += pred.sum()\n",
    "        n_steps += 1\n",
    "    \n",
    "    target_mean = target_sum/n_steps\n",
    "    pred_mean = pred_sum/n_steps\n",
    "    \n",
    "    mse = 0\n",
    "    mean_error = 0\n",
    "    cov = 0\n",
    "    abs_log_ratio = 0\n",
    "    \n",
    "    var_pred = 0 #technically var * N but gets cancelled out in Pearson calculation\n",
    "    var_target = 0 \n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        mse += MSE(pred, y)\n",
    "        mean_error += Bias(pred, y)\n",
    "        cov += CovStep(pred, y, pred_mean, target_mean)\n",
    "        abs_log_ratio += MeanAbsLogStep(pred, y, log=log)\n",
    "        \n",
    "        var_pred += MSE(pred, pred_mean)\n",
    "        var_target += MSE(y, target_mean)\n",
    "        \n",
    "        pears = CovStep(pred, y, pred_mean, target_mean)/(torch.sqrt(MSE(pred, pred_mean))*torch.sqrt(MSE(y, target_mean)))\n",
    "    \n",
    "    out_dict = {}\n",
    "    out_dict['mse'] = (mse / n_steps).item()\n",
    "    out_dict['bias'] = (mean_error / n_steps).item()\n",
    "    out_dict['pearson_cor'] = (cov/(torch.sqrt(var_pred) * torch.sqrt(var_target))).item()\n",
    "    out_dict['mean_mult'] = (torch.exp(abs_log_ratio/n_steps)).item()\n",
    "    \n",
    "    return out_dict\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     eval_dict = compute_eval_metrics(val_dataloader, model)\n",
    "#     print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7ca9da7-3de4-4f84-a11e-2e7f123691ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n",
      "torch.Size([16, 1])\n",
      "torch.Size([16, 1])\n",
      "torch.Size([16, 1])\n",
      "torch.Size([16, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     30\u001b[0m train_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/s3d_env/lib/python3.8/site-packages/torch/optim/adam.py:261\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    258\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    264\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = Adam(model.parameters(),lr=0.000005, weight_decay=1e-2)\n",
    "\n",
    "hist = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"val_loss\": [],\n",
    "    \"val_bias\": [],\n",
    "    \"val_pearson_cor\": [],\n",
    "    \"val_mean_mult\": []\n",
    "}\n",
    "\n",
    "for ep in range(20):     #########################################################################                   \n",
    "    t_start = time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_steps = 0\n",
    "    val_steps = 0\n",
    "    \n",
    "    for (x, y) in train_dataloader:\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        loss = MSE(pred, y.reshape((y.shape[0], 1)))\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_steps += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        val_metrics = compute_eval_metrics(val_dataloader, model)\n",
    "    \n",
    "    \n",
    "    hist['train_loss'].append(train_loss/train_steps)\n",
    "    hist['val_loss'].append(val_metrics['mse'])\n",
    "    hist['val_bias'].append(val_metrics['bias'])\n",
    "    hist['val_pearson_cor'].append(val_metrics['pearson_cor'])\n",
    "    hist['val_mean_mult'].append(val_metrics['mean_mult'])\n",
    "    \n",
    "    t_end = time()\n",
    "    \n",
    "    t_elapsed = t_end - t_start\n",
    "    print(\"Epoch: {}\\tDuration: {:.2f}s\\tTrain loss: {:.4f}\\tVal loss: {:.4f}\\tVal bias:{:.4f}\\tVal Pearson correlation: {:.4e}\\tVal MeanMult: {:.4f}\"\\\n",
    "          .format(ep, t_elapsed, train_loss/train_steps, val_metrics['mse'],\\\n",
    "                  val_metrics['bias'], val_metrics['pearson_cor'],val_metrics['mean_mult']))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 创建一个空列表来存储pred和y的值\n",
    "data_to_save = []\n",
    "\n",
    "test1_df = test_df.sample(5)\n",
    "\n",
    "mae = 0.0 \n",
    "total_samples = 0\n",
    "\n",
    "test_dataloader1 = create_dataloader(test1_df) \n",
    "test_random = compute_eval_metrics(test_dataloader1,model) \n",
    "print(test_random)\n",
    "\n",
    "for(x,y) in test_dataloader: \n",
    "    (x,y) = (x.to(device),y.to(device)) \n",
    "    pred = model(x) \n",
    "    for i in range(len(pred)):\n",
    "        data_to_save.append([pred[i].item(), y[i].item()])\n",
    "\n",
    "# 指定要保存的CSV文件名\n",
    "csv_filename = 'predictions.csv'\n",
    "\n",
    "# 打开CSV文件并将数据写入\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # 写入列名（如果需要）\n",
    "    csv_writer.writerow(['Prediction', 'Actual'])\n",
    "    \n",
    "    # 写入数据\n",
    "    csv_writer.writerows(data_to_save)\n",
    "\n",
    "print(f'Data saved to {csv_filename}')\n",
    "#     print(pred,'///',y)\n",
    "\n",
    "#     # 计算绝对误差\n",
    "#     absolute_error = torch.abs(pred-y)\n",
    "\n",
    "#     # 累加绝对误差和样本数\n",
    "#     mae += absolute_error.sum().item()\n",
    "    \n",
    "\n",
    "# #     计算平均绝对误差\n",
    "\n",
    "\n",
    "# mae /= 5 \n",
    "# print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb35a4-183c-43d8-ab9a-00adaba3bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_test = compute_eval_metrics(test_dataloader, model)\n",
    "    print(eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7365bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd7026-863e-496b-bf18-0c22f39c92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.keys()\n",
    "np.std(hist['val_loss'][15:])\n",
    "np.arange(100)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f67288-f36f-431b-8482-e6d4a76d29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_dataloader = create_dataloader(random_df)\n",
    "    eval_random = compute_eval_metrics(random_dataloader, model)\n",
    "    print(eval_random)\n",
    "\n",
    "    for (x, y) in random_dataloader:\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            pred = model(x)\n",
    "            print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c8c5-5077-4c39-99dd-e3ab55dba281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class print_Model(Module):\n",
    "    def __init__(self, seq):\n",
    "        super(print_Model, self).__init__()\n",
    "        self.net = seq\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Start\\n{}\".format(x.size()))\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            print(layer)\n",
    "            print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acb0b5-cf2a-4487-9527-7669b605568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b10cc-e81a-4238-b988-3600a5b6554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "random_dataloader = create_dataloader(random_df, log=False)\n",
    "\n",
    "\n",
    "load_model = Baseline_Model((input_height, input_width)).to(device)\n",
    "model_name = 'testrun2_regularization'\n",
    "load_model.load_state_dict(torch.load(os.path.join(MODELS_DIR,model_name,'model_state.pt'), map_location=torch.device('cpu')))\n",
    "\n",
    "load_metrics = compute_eval_metrics(test_dataloader, load_model, log=False)\n",
    "for key in load_metrics.keys():\n",
    "    print(key, load_metrics[key])\n",
    "\n",
    "for (x, y) in random_dataloader:\n",
    "    (x, y) = (x.to(device), y.to(device))\n",
    "    pred = load_model(x)\n",
    "    print(pred.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469273db-2880-4c15-a8c5-834d1a0e9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df['vol'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555aab10",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxwnotebook",
   "language": "python",
   "name": "cxwnotebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
